version: '3.8'

services:
  # Qdrant Vector Database Service
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: always
    ports:
      - "${QDRANT_URL_PORT:-6333}:6333" # Map host port to container port for HTTP
      - "6334:6334" # gRPC port
    environment:
      # Set the API key from the .env file
      - QDRANT__SERVICE__API_KEY=${QDRANT_API_KEY}
    volumes:
      - ./qdrant_storage:/qdrant/storage
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Qwen3 OpenAI-Compatible API Service
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: qwen3-api
    restart: always
    ports:
      - "${API_PORT:-8000}:${API_PORT:-8000}"
    env_file:
      - .env
    volumes:
      # Mount the model file and templates into the container
      # This allows the container to access the GGUF model optimized by optimize_gguf.py
      - ./${OLLAMA_MODEL_NAME}.gguf:./${OLLAMA_MODEL_NAME}.gguf
      - ./instruction_templates:/app/instruction_templates
    depends_on:
      qdrant:
        condition: service_healthy
      # Note: This setup assumes Ollama is running on the host machine,
      # as Docker on macOS/Windows has networking limitations with host.docker.internal.
      # The API inside the container will connect to Ollama on the host.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${API_PORT:-8000}/health"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  qdrant_storage: 